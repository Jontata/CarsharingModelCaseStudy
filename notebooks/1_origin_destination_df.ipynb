{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import networkx as nx # type: ignore\n",
    "import geopandas as gpd # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon # type: ignore\n",
    "from typing import Tuple\n",
    "\n",
    "# Set working directory\n",
    "while os.path.basename(os.getcwd()).lower() != 'carsharingmodelcasestudy':\n",
    "    os.chdir('..')\n",
    "assert os.path.basename(os.getcwd()).lower() == 'carsharingmodelcasestudy', os.getcwd()\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.mode.chained_assignment = None  # Suppress SettingWithCopyWarning\n",
    "pd.set_option('display.max_columns', None) # Show all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrosm # type: ignore\n",
    "import osmnx as ox # type: ignore\n",
    "self = pyrosm.OSM.__init__ # Initialize the OSM object \n",
    "osm = pyrosm.OSM(\"./data/OSM/Copenhagen.osm.pbf\")\n",
    "# get stations from the data\n",
    "data_folder = 'data/'\n",
    "stations = pd.read_csv(data_folder + '20_css_cop_latlng.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get walking network\n",
    "walk_net = osm.get_network(network_type=\"walking\", nodes=True)\n",
    "walk_nodes, walk_edges = walk_net\n",
    "G_walk = osm.to_graph(walk_nodes, walk_edges, graph_type=\"networkx\")\n",
    "# Get POIs\n",
    "pois = osm.get_pois()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing POIs based on purpose\n",
    "\n",
    "We have analyzed the tags and created the following groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "commute_1_shop = pois[\"shop\"].value_counts().index.to_list() # Work\n",
    "commute_2_amenity = [\"school\", \"college\", \"university\"] # Education\n",
    "errands_1 = [] # Home, perm. residence\n",
    "errands_2_amenity = [\"bus_station\", ] # Escorting to/from transport\n",
    "errands_2_tags = [{\"public_transport\": \"station\"}]\n",
    "errands_3 = [] # Collect/bring objects\n",
    "errands_4_shop = pois[\"shop\"].value_counts().index.to_list() # Shopping\n",
    "errands_5_amenity = [\"hospital\", \"pharmacy\"] # Health\n",
    "leisure_1 = [] # Home\n",
    "leisure_2_ammenity = ['school', 'community_centre', 'library', 'arts_centre', 'university', 'college'] # After-school, youth club\n",
    "leisure_3_ammenity = ['kindergarten', 'community_centre', 'social_facility'] # Nursery, cr√©che, day care\n",
    "leisure_4 = [] # Visit family/friends\n",
    "leisure_5_ammenity = ['park'] # Sports\n",
    "leisure_6_ammenity = ['place_of_worship', 'theatre', 'cinema', 'nightclub', 'casino', 'music_venue', 'hookah_lounge', 'gambling'] # Entertainment\n",
    "leisure_6_religion = ['christian', 'muslim', 'jewish', 'buddhist', 'scientologist']\n",
    "leisure_6_museum = ['art', 'history', 'local', 'military']\n",
    "leisure_7 = [] # Allotment/summer cottage\n",
    "leisure_8 = [] # Leisure round trip\n",
    "leisure_8_tourism = ['museum', 'attraction', 'viewpoint']\n",
    "leisure_9_ammenity = ['restaurant', 'cafe', 'pub'] # Holiday, excursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new column which holds lists of relevant groups\n",
    "pois['relevant_groups'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_relevant_group_column(pois_df, column_name, list_values, append_group):\n",
    "    # Create a boolean column indicating whether the condition is met\n",
    "    pois_df['isin_bool'] = pois_df[column_name].isin(list_values)\n",
    "    # Update the 'relevant_groups' column\n",
    "    pois_df['relevant_groups'] = pois_df.apply(\n",
    "        lambda row: list(set((row['relevant_groups'] if isinstance(row['relevant_groups'], list) else []) + [append_group]))\n",
    "        if row['isin_bool'] else row['relevant_groups'],\n",
    "        axis=1\n",
    "    )\n",
    "    #print(len(pois_df[pois_df['relevant_groups'].notnull()]['relevant_groups']))\n",
    "    return pois_df.drop(columns=['isin_bool'])\n",
    "\n",
    "pois = update_relevant_group_column(pois, 'shop', commute_1_shop, 'commute_1_shop')\n",
    "pois = update_relevant_group_column(pois, 'amenity', commute_2_amenity, 'commute_2_amenity')\n",
    "pois = update_relevant_group_column(pois, 'amenity', errands_2_amenity, 'errands_2_amenity')\n",
    "pois = update_relevant_group_column(pois, 'shop', errands_4_shop, 'errands_4_shop')\n",
    "pois = update_relevant_group_column(pois, 'amenity', errands_5_amenity, 'errands_5_amenity')\n",
    "pois = update_relevant_group_column(pois, 'amenity', leisure_2_ammenity, 'leisure_2_ammenity')\n",
    "pois = update_relevant_group_column(pois, 'amenity', leisure_3_ammenity, 'leisure_3_ammenity')\n",
    "pois = update_relevant_group_column(pois, 'amenity', leisure_5_ammenity, 'leisure_5_ammenity')\n",
    "pois = update_relevant_group_column(pois, 'amenity', leisure_6_ammenity, 'leisure_6_ammenity')\n",
    "pois = update_relevant_group_column(pois, 'religion', leisure_6_religion, 'leisure_6_religion')\n",
    "pois = update_relevant_group_column(pois, 'museum', leisure_6_museum, 'leisure_6_museum')\n",
    "pois = update_relevant_group_column(pois, 'tourism', leisure_8_tourism, 'leisure_8_tourism')\n",
    "pois = update_relevant_group_column(pois, 'amenity', leisure_9_ammenity, 'leisure_9_ammenity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle errands_2_tags we need to parse the tags column and then update the relevant_groups\n",
    "# Parse tags to dictionary\n",
    "def parse_tags(x):\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    elif isinstance(x, str):\n",
    "        try:\n",
    "            return eval(x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create a list of indices where the tag \"public_transport\" is in the tags and the value is \"station\"\n",
    "pois['parsed_tags'] = pois['tags'].apply(parse_tags)\n",
    "\n",
    "# Update the 'relevant_groups' column for rows where 'parsed_tags' contains {'public_transport': 'station'}, \n",
    "pois['relevant_groups'] = pois.apply(\n",
    "    lambda row: (\n",
    "        list(set((row['relevant_groups'] if isinstance(row['relevant_groups'], list) else []) + [\"errands_2_tags\"]))\n",
    "        if isinstance(row['parsed_tags'], dict) and row['parsed_tags'].get('public_transport') == 'station'\n",
    "        else row['relevant_groups']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "print(len(pois[pois['relevant_groups'].notnull()]['relevant_groups']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to keep the rows where 'relevant_groups' is not None\n",
    "pois_filtered = pois[pois['relevant_groups'].notnull()]\n",
    "\n",
    "# Create controids for POIs\n",
    "def get_centroid(geometry):\n",
    "    if isinstance(geometry, Point):\n",
    "        return geometry\n",
    "    elif isinstance(geometry, (Polygon, MultiPolygon)):\n",
    "        return geometry.centroid\n",
    "\n",
    "# Filter out None values in the 'centroid' column\n",
    "pois_filtered['centroid'] = pois_filtered['geometry'].apply(get_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pois_filtered))\n",
    "pois_filtered = pois_filtered[pois_filtered['centroid'].notnull()]\n",
    "print(len(pois_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_filtered['centroid_lon'] = pois_filtered['centroid'].apply(lambda point: point.x)\n",
    "pois_filtered['centroid_lat'] = pois_filtered['centroid'].apply(lambda point: point.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GeoDataFrames for POIs and stations\n",
    "pois_gdf = gpd.GeoDataFrame(\n",
    "    pois_filtered,\n",
    "    geometry=gpd.points_from_xy(pois_filtered['centroid_lon'], pois_filtered['centroid_lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations,\n",
    "    geometry=gpd.points_from_xy(stations['lng'], stations['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "poi_coords = list(zip(pois_gdf.geometry.x, pois_gdf.geometry.y))\n",
    "station_coords = list(zip(stations_gdf.geometry.x, stations_gdf.geometry.y))\n",
    "\n",
    "poi_nodes = ox.distance.nearest_nodes(G_walk, X=[x for x, y in poi_coords], Y=[y for x, y in poi_coords])\n",
    "station_nodes = ox.distance.nearest_nodes(G_walk, X=[x for x, y in station_coords], Y=[y for x, y in station_coords])\n",
    "\n",
    "station_node_set = set(station_nodes)\n",
    "distance_threshold = 1000\n",
    "filtered_pois = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each POI node to compute distances to stations\n",
    "for idx, poi_node in enumerate(poi_nodes):\n",
    "    # Compute shortest path lengths from the POI node to all other nodes within the threshold\n",
    "    lengths = nx.single_source_dijkstra_path_length(G_walk, poi_node, cutoff=distance_threshold, weight='length')\n",
    "    \n",
    "    # Find the minimum distance to any station node\n",
    "    min_distance = float('inf')\n",
    "    for station_node in station_node_set:\n",
    "        if station_node in lengths:\n",
    "            distance = lengths[station_node]\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                \n",
    "    # If the minimum distance is within the threshold, keep the POI\n",
    "    if min_distance < distance_threshold:\n",
    "        filtered_pois.append(pois_filtered.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the filtered_pois to create a dataframe\n",
    "filtered_pois_df = pd.DataFrame(filtered_pois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace leisure_6_ammenity, leisure_6_religion, leisure_6_museum with leisure_6\n",
    "filtered_pois_df['relevant_groups'] = filtered_pois_df['relevant_groups'].apply(\n",
    "    lambda x: ['leisure_6'] if 'leisure_6_ammenity' in x or 'leisure_6_religion' in x or 'leisure_6_museum' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot stations and POIs\n",
    "def plot_stations_pois(stations, pois_filtered):\n",
    "    # Convert stations and POIs to GeoDataFrames if they aren't already\n",
    "    stations_gdf = gpd.GeoDataFrame(stations, geometry=gpd.points_from_xy(stations['lng'], stations['lat']), crs=\"EPSG:4326\")\n",
    "    pois_filtered_gdf = gpd.GeoDataFrame(pois_filtered, geometry=gpd.points_from_xy(pois_filtered['centroid_lon'], pois_filtered['centroid_lat']), crs=\"EPSG:4326\")\n",
    "    # Convert the 'relevant_groups' column to tuples, create a 'primary_category' column, and get unique categories\n",
    "    pois_filtered_gdf['relevant_groups'] = pois_filtered_gdf['relevant_groups'].apply(tuple)\n",
    "    pois_filtered_gdf['primary_category'] = pois_filtered_gdf['relevant_groups'].apply(lambda x: x[0])\n",
    "    categories = pois_filtered_gdf['primary_category'].unique()\n",
    "    # Create a colormap\n",
    "    cmap_name = \"Paired\"\n",
    "    cmap = matplotlib.colormaps[cmap_name]\n",
    "    num_categories = len(categories)\n",
    "    colors = cmap.colors[:num_categories] if hasattr(cmap, 'colors') else cmap(range(num_categories))\n",
    "    listed_cmap = matplotlib.colors.ListedColormap(colors)\n",
    "    category_to_color = {category: listed_cmap(i) for i, category in enumerate(categories)}\n",
    "    pois_filtered_gdf['color'] = pois_filtered_gdf['primary_category'].map(category_to_color)\n",
    "    # Plot the map\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    # Plot the road network as a background\n",
    "    drive_net = osm.get_network(network_type=\"driving\", nodes=True)\n",
    "    drive_nodes, drive_edges = drive_net\n",
    "    drive_edges.plot(ax=ax, linewidth=0.5, color=\"gray\", alpha=0.4)\n",
    "    # Plot filtered POIs with another color and marker\n",
    "    pois_filtered_gdf.plot(ax=ax, color=pois_filtered_gdf['color'], marker=\"o\", markersize=5, alpha=0.5, label=\"POIs\")\n",
    "    # Plot stations with a specific color and marker\n",
    "    stations_gdf.plot(ax=ax, color=\"blue\", marker=\"o\", markersize=45, label=\"Stations\")\n",
    "    # Set the limits of the plot\n",
    "    x_min, x_max = 12.35, 12.75\n",
    "    y_min, y_max = 55.585, 55.785\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    # Add legend and title\n",
    "    plt.legend()\n",
    "    #plt.title(\"Stations and Filtered POIs in Copenhagen\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    # Set a dark background\n",
    "    background_color = \"#FCFAFC\" #00111A\n",
    "    ax.set_facecolor(background_color) \n",
    "    fig.patch.set_facecolor((0, 0, 0, 0))\n",
    "    plt.savefig('LaTeX/images/stations_with_pois_near.png', format='png', dpi=300, transparent=False)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your stations and filtered POIs data\n",
    "plot_stations_pois(stations, filtered_pois_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic traveller origin-destination data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = filtered_pois_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df_lats = nodes_df['centroid_lat'].values\n",
    "nodes_df_lons = nodes_df['centroid_lon'].values\n",
    "nodes_df['node'] = ox.distance.nearest_nodes(G_walk, nodes_df_lons, nodes_df_lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df[\"relevant_groups\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df['relevant_groups'] = nodes_df['relevant_groups'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace leisure_6_ammenity, leisure_6_religion, leisure_6_museum with leisure_6\n",
    "nodes_df['relevant_groups'] = nodes_df['relevant_groups'].apply(\n",
    "    lambda x: ['leisure_6'] if 'leisure_6_ammenity' in x or 'leisure_6_religion' in x or 'leisure_6_museum' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_probabilities = {\n",
    "    'commute_1_shop': 0.134,\n",
    "    'commute_2_amenity': 0.049,\n",
    "    'errands_2_amenity': 0.01,\n",
    "    'errands_4_shop': 0.196,\n",
    "    'errands_5_amenity': 0.034,\n",
    "    'leisure_2_ammenity': 0.03,\n",
    "    'leisure_3_ammenity': 0.02,\n",
    "    'leisure_5_ammenity': 0.034,\n",
    "    'leisure_6': 0.058,\n",
    "    'leisure_8_tourism': 0.16,\n",
    "    'leisure_9_ammenity': 0.036\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping group names to POI nodes\n",
    "group_to_nodes = {\n",
    "    group: nodes_df[nodes_df['relevant_groups'].apply(lambda x: group in x)]\n",
    "              [['node', 'centroid_lon', 'centroid_lat']]\n",
    "              .values.tolist()\n",
    "    for group in group_probabilities\n",
    "}\n",
    "# Remove empty groups\n",
    "group_to_nodes = {k: v for k, v in group_to_nodes.items() \n",
    "        if len(v) > 0}\n",
    "groups = [group_name for group_name in group_probabilities.keys() \n",
    "        if group_name in group_to_nodes.keys()]\n",
    "# Probability of selecting each group\n",
    "probabilities = [group_probabilities[group] for group in groups]\n",
    "probabilities = np.array(probabilities)\n",
    "probabilities = probabilities / probabilities.sum()  # Normalize\n",
    "cumulative_probabilities = np.cumsum(probabilities)\n",
    "# As an example, a single trip destination can be drawn as follows\n",
    "trip_group = groups[np.searchsorted(cumulative_probabilities, random.random())]\n",
    "trip_destination = random.choice(group_to_nodes.get(trip_group, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the length of each group\n",
    "for group, nodes in group_to_nodes.items():\n",
    "    print(f\"{group}: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groups))\n",
    "print(groups)\n",
    "print(probabilities)\n",
    "print(cumulative_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walking_distance(origin_node, destination_node):\n",
    "    try:\n",
    "        length = nx.shortest_path_length(G_walk, origin_node, destination_node, weight='length')\n",
    "    except nx.NetworkXNoPath:\n",
    "        length = 0\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trips = 2500\n",
    "origins = []\n",
    "destinations = []\n",
    "trip_groups = []\n",
    "\n",
    "for _ in range(num_trips):\n",
    "    # Get groups for origin and destination nodes\n",
    "    origin_rand = random.random()\n",
    "    origin_group_index = np.searchsorted(cumulative_probabilities, origin_rand)\n",
    "    origin_group = groups[origin_group_index]\n",
    "    dest_rand = random.random()\n",
    "    dest_group_index = np.searchsorted(cumulative_probabilities, dest_rand)\n",
    "    destination_group = groups[dest_group_index]\n",
    "\n",
    "    # Get nodes based on groups\n",
    "    origin_nodes = group_to_nodes.get(origin_group, [])\n",
    "    dest_nodes = group_to_nodes.get(destination_group, [])\n",
    "\n",
    "    # Ensure nodes are available in both groups\n",
    "    if not origin_nodes:\n",
    "        print(f\"No nodes available for group {origin_group}\")\n",
    "        raise ValueError(\"No nodes available for origin group\")\n",
    "    if not dest_nodes:\n",
    "        print(f\"No nodes available for group {destination_group}\")\n",
    "        raise ValueError(\"No nodes available for destination group\")\n",
    "        \n",
    "\n",
    "    # Select origin and destination nodes from groups\n",
    "    distance_between_nodes = 0\n",
    "    while distance_between_nodes < 1000: # Ensure the distance is at least 1 km\n",
    "        origin = random.choice(origin_nodes)\n",
    "        destination = random.choice(dest_nodes)\n",
    "        print(f\"Origin: {origin}\")\n",
    "        print(f\"Destination: {destination}\")\n",
    "        distance_between_nodes = walking_distance(origin[0], destination[0])\n",
    "        print(f\"Distance between nodes: {distance_between_nodes} (node {_} of {num_trips})\")\n",
    "\n",
    "    # Record the trip data\n",
    "    origins.append(origin)\n",
    "    destinations.append(destination)\n",
    "    trip_groups.append((origin_group, destination_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OD DataFrame\n",
    "od_data = pd.DataFrame({\n",
    "    \"origin_lon\": [x[1] for x in origins],\n",
    "    \"origin_lat\": [x[2] for x in origins],\n",
    "    \"destination_lon\": [x[1] for x in destinations],\n",
    "    \"destination_lat\": [x[2] for x in destinations],\n",
    "    \"trip_group\": trip_groups[:len(origins)],\n",
    "    \"trip_id\": np.arange(1, len(origins)+1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the od_data to a csv file\n",
    "od_data.to_csv('requests/od_data_2500.csv', sep=';', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
